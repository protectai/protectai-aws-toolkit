{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secure Model Import Using Protect AI Guardian to bring DeepSeek-R1-Distill-Llama Models to Amazon Bedrock\n",
    "\n",
    "This notebook demonstrates how to securely import DeepSeek's distilled Llama models to Amazon Bedrock using Custom Model Import (CMI). We'll use the 8B parameter model as an example, <u>but the same process applies to the 70B variant</u>. Throughout this process, we'll implement security best practices with Protect AI Guardian to ensure our model is free from vulnerabilities before deployment.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As you integrate Generative AI into enterprise workflows, you unlock tremendous innovation potential but also face significant security challenges. Open source models, in particular, may contain hidden vulnerabilities including data leakage risks and susceptibility to adversarial attacks. To scale AI both rapidly and securely, it's critical to assess and mitigate these threats before deployment.\n",
    "\n",
    "DeepSeek has released several distilled versions of their models based on Llama architecture. These models maintain strong performance while being more efficient, but like any third-party model, should be properly scanned for security issues. The 8B model we'll use here is derived from Llama 3.1 and has been **optimized for reasoning tasks**.\n",
    "\n",
    "## Protect AI Guardian\n",
    "\n",
    "Guardian secures your machine learning ecosystem through two integrated components that seamlessly connect to your existing MLOps pipelines:\n",
    "\n",
    "- **Guardian Gateway:** This proxy service creates a secure perimeter around third-party model access by intercepting requests to sources like Hugging Face Hub.\n",
    "  \n",
    "- **Guardian Scanner:** Deployed within your infrastructure, Scanner provides a dedicated API endpoint for validating your internal first-party models as they move through development or undergo customization.\n",
    "\n",
    "In this notebook, we'll demonstrate how Guardian Gateway and Guardian Scanner seamlessly integrate into your existing ML pipeline to establish a secure, repeatable workflow that protects both third-party and internally developed models throughout their lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb1b2d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Appropriate IAM roles and permissions for Bedrock and Amazon S3, follow [the instructions here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-import-iam-role.html)\n",
    "- A S3 bucket prepared to store the custom model\n",
    "- Sufficient local storage space (At least 17GB for 8B and 135GB for 70B models)\n",
    "- Reach out to the [Protect AI team](https://protectai.com/contact-sales) to get access to Guardian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15844386-cbc9-49c9-8dbe-fcbf2dfab1eb",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Packages\n",
    "\n",
    "First, let's install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e0984-0d83-4e83-8710-8d4870444af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install boto3 --upgrade\n",
    "!pip install -U huggingface_hub\n",
    "!pip install hf_transfer huggingface huggingface_hub \"huggingface_hub[hf_transfer]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec46496-b12a-407e-8ad5-bf4e60a7bf97",
   "metadata": {},
   "source": [
    "### Step 2: Configure Parameters\n",
    "\n",
    "Update these parameters according to your AWS environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd8cb8-d3c0-4e6c-ba58-071cdee2894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters (please update this part based on your setup)\n",
    "bucket_name = \"<YOUR-PREDEFINED-S3-BUCKET-TO-HOST-IMPORT-MODEL>\"\n",
    "s3_prefix = \"<S3-PREFIX>\" # E.x. DeepSeek-R1-Distill-Llama-8B\n",
    "local_directory = \"<LOCAL-FOLDER-TO-STORE-DOWNLOADED-MODEL>\" # E.x. DeepSeek-R1-Distill-Llama-8B\n",
    "\n",
    "job_name = '<CMI-JOB-NAME>' # E.x. Deepseek-8B-job\n",
    "imported_model_name = '<CMI-MODEL-NAME>' # E.x. Deepseek-8B-model\n",
    "role_arn = '<IAM-ROLE-ARN>' # Please make sure it has sufficient permission as listed in the pre-requisite\n",
    "\n",
    "# Region (currently only 'us-west-2' and 'us-east-1' support CMI with Deepseek-Distilled-Llama models)\n",
    "region_info = 'us-west-2' # You can modify to 'us-east-1' based on your need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30527593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Enable hf_transfer for faster downloads\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Guardian Gateway endpoint provided by your Protect AI representative\n",
    "os.environ[\"HF_ENDPOINT\"]=\"<YOUR_GUARDIAN_GATEWAY_ENDPOINT>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffabe2-0c4b-431a-aadb-167f5cb26fa4",
   "metadata": {},
   "source": [
    "### Step 3: Download Model from Hugging Face\n",
    "\n",
    "Download the model files from Hugging Face. \n",
    "\n",
    "- Note that you can also use the 70B model by changing the model_id to \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7789f-958b-4459-a345-67891c5c86ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Downloading the 8B model files may take 2-10 minutes depending on your internet connection speed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any existing model files that may already exist\n",
    "!rm -r ~/.cache/huggingface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e91e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the endpoint is set to the Guardian Gateway\n",
    "print(os.environ[\"HF_ENDPOINT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12e1c1-2aec-4fda-9199-a32d95d2e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "hf_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# Download using snapshot_download with hf_transfer enabled\n",
    "snapshot_download(repo_id=hf_model_id, local_dir=f\"./{local_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d2428",
   "metadata": {},
   "source": [
    "Upon successful download, navigate to the Guardian dashboard to verify that the model was scanned and review the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557e7ac-0626-4b9a-ba62-d2207e25cd23",
   "metadata": {},
   "source": [
    "### Step 4: Upload Model to S3\n",
    "\n",
    "Upload the scanned model files to your S3 bucket\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Uploading the 8B model files normally takes 10-20 minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed56d4-ee79-4378-8a77-105889f840b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upload_directory_to_s3(local_directory, bucket_name, s3_prefix):\n",
    "    s3_client = boto3.client('s3')\n",
    "    local_directory = Path(local_directory)\n",
    "    \n",
    "    # Get list of all files first\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            local_path = Path(root) / filename\n",
    "            relative_path = local_path.relative_to(local_directory)\n",
    "            s3_key = f\"{s3_prefix}/{relative_path}\"\n",
    "            all_files.append((local_path, s3_key))\n",
    "    \n",
    "    # Upload with progress bar\n",
    "    for local_path, s3_key in tqdm(all_files, desc=\"Uploading files\"):\n",
    "        try:\n",
    "            s3_client.upload_file(\n",
    "                str(local_path),\n",
    "                bucket_name,\n",
    "                s3_key\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {local_path}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Upload all files\n",
    "upload_directory_to_s3(local_directory, bucket_name, s3_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46757ca1",
   "metadata": {},
   "source": [
    "### Step 5: Simulate a Malicious Attack\n",
    "We're going to add a \"vulnerability\" to our existing model by adding a file called extra_data.pkl in the model directory and uploading it to S3. This file is not part of the original model and should be flagged by Guardian Scanner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class MaliciousPayload:\n",
    "    def __reduce__(self):\n",
    "        # Harmless demo payload (prints a message)\n",
    "        return (os.system, ('echo \"Security vulnerability demonstration\"',))\n",
    "\n",
    "# Create malicious pickle\n",
    "with open('./extra_data.pkl', 'wb') as f:\n",
    "    pickle.dump(MaliciousPayload(), f)\n",
    "\n",
    "# Upload the malicious pickle\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(\n",
    "    './extra_data.pkl',\n",
    "    bucket_name,\n",
    "    f\"{s3_prefix}/extra_data.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6863472",
   "metadata": {},
   "source": [
    "### Step 6: Use Guardian Scanner to Detect the new \"Vulnerability\"\n",
    "Load the Guardian Scanner environment variables to begin scanning the model stored in S3 for vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install guardian-client from PyPI\n",
    "%pip install guardian-client==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Please reach out to your Protect AI representative to retrieve the proper environment variables\n",
    "load_dotenv(\"./.env.local\", override=True)  # Path is relative to current notebook path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5df932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the endpoint of the Guardian Scanner's API\n",
    "scanner_endpoint = os.environ[\"GUARDIAN_SCANNER_ENDPOINT\"]\n",
    "\n",
    "# Set the model URI path\n",
    "model_uri = (\n",
    "    f\"s3://{bucket_name}/{s3_prefix}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Guardian API Client\n",
    "from guardian_client import GuardianAPIClient\n",
    "\n",
    "# Initiate the client using the GUARDIAN_SCANNER_ENDPOINT that we set above\n",
    "guardian = GuardianAPIClient(base_url=scanner_endpoint)\n",
    "\n",
    "# # Scan the model\n",
    "response = guardian.scan(model_uri=model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the pass/fail decision from Guardian\n",
    "assert response.get(\"http_status_code\") == 200\n",
    "assert response.get(\"scan_status_json\") != None\n",
    "assert response.get(\"scan_status_json\").get(\"aggregate_eval_outcome\") != \"ERROR\"\n",
    "\n",
    "if response.get(\"scan_status_json\").get(\"aggregate_eval_outcome\") == \"FAIL\":\n",
    "    print(\n",
    "        f\"Model {model_uri} was blocked because it failed your organization's security policies\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d609982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce02981",
   "metadata": {},
   "source": [
    "### Step 7: Take Action on the Detected Vulnerability and Re-Scan\n",
    "After scanning the model, you can view the results in the Guardian dashboard. If a vulnerability is detected, you can take action to mitigate the risk. \n",
    "\n",
    "**We will simply remove the extra_data.pkl file in this example, but in a real-world scenario, you would address the underlying issue based on your organization's security policies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the malicious pickle\n",
    "s3_client.delete_object(Bucket=bucket_name, Key=f\"{s3_prefix}/extra_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7b58f",
   "metadata": {},
   "source": [
    "Re-scan the model to ensure the vulnerability has been resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9446664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan the model after deleting the malicious file. \n",
    "response = guardian.scan(model_uri=model_uri)\n",
    "\n",
    "# Retrieve the pass/fail decision from Guardian\n",
    "assert response.get(\"http_status_code\") == 200\n",
    "assert response.get(\"scan_status_json\") != None\n",
    "assert response.get(\"scan_status_json\").get(\"aggregate_eval_outcome\") != \"ERROR\"\n",
    "\n",
    "if response.get(\"scan_status_json\").get(\"aggregate_eval_outcome\") == \"FAIL\":\n",
    "    print(\n",
    "        f\"Model {model_uri} was blocked because it failed your organization's security policies\"\n",
    "    )\n",
    "\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3042657-a7e1-4063-abb5-73ea1f8cda80",
   "metadata": {},
   "source": [
    "### Step 5: Create Custom Model Import Job\n",
    "\n",
    "Initialize the import job in Amazon Bedrock\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Creating CMI job for 8B model could take 5-20 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f898ec5-6123-4647-8852-456045efcada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client('bedrock', region_name=region_info)\n",
    "\n",
    "s3_uri = f's3://{bucket_name}/{s3_prefix}/'\n",
    "print(role_arn)\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_uri\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a18c49-e5a9-4706-abe5-f11e1dfa4c3a",
   "metadata": {},
   "source": [
    "### Step 6: Monitor Import Job Status\n",
    "\n",
    "Check the status of your import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4235f3-4136-43a8-b461-f7c4e96e174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "\n",
    "# Get the model ID\n",
    "model_id = response['importedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612d199-ffcf-4ffd-803e-b05e1c148f88",
   "metadata": {},
   "source": [
    "### Step 7: Wait for Model Initialization\n",
    "\n",
    "Allow time for the model to initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465e3ed-4cb8-4e9e-9740-3e972fec8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for 5mins for cold start \n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319747f-4b8f-41b7-abf2-a046e0a23d51",
   "metadata": {},
   "source": [
    "### Step 8: Model Inference with Proper Tokenization\n",
    "\n",
    "#### Understanding the Tokenization Process\n",
    "When working with DeepSeek models, proper tokenization is crucial for optimal performance. The model expects inputs to follow a specific format defined in its `tokenizer_config.json`. This format ensures the model receives prompts in the same structure it was trained on.\n",
    "\n",
    "#### Key Components\n",
    "1. **Tokenizer**: Uses HuggingFace's AutoTokenizer to properly format inputs\n",
    "2. **Generation Function**: Handles the core interaction with the model\n",
    "3. **Auto-Generate Function**: Manages longer responses that might exceed token limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce88862d-930e-4721-84d4-46705c2b7908",
   "metadata": {},
   "source": [
    "#### 8.1 Setting Up the Tokenizer\n",
    "First, we'll initialize the tokenizer and Bedrock runtime client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995cabd-9148-4ce8-b458-c0921130a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id)\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region_info,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,     # 5 minutes\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a8b29-6110-4874-9c76-264fb89b188e",
   "metadata": {},
   "source": [
    "#### 8.2 Core Generation Function\n",
    "\n",
    "This function handles the basic model interaction with proper tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f07be7-7fa4-4deb-84c8-ec1de9829d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, temperature=0.3, max_tokens=4096, top_p=0.9, continuation=False, max_retries=10):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "    \n",
    "    Parameters:\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        continuation (bool): Whether this is a continuation of previous generation\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, \n",
    "                                         add_generation_prompt=not continuation)\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=json.dumps({\n",
    "                    'prompt': prompt,\n",
    "                    'temperature': temperature,\n",
    "                    'max_gen_len': max_tokens,\n",
    "                    'top_p': top_p\n",
    "                }),\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['body'].read().decode('utf-8'))\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(30)\n",
    "    \n",
    "    raise Exception(\"Failed to get response after maximum retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51f911-7368-4244-8291-11ec18fb031a",
   "metadata": {},
   "source": [
    "#### 8.3 Extended Generation Function\n",
    "\n",
    "The thinking process of the model can become quite extensive, especially when dealing with complex reasoning problems that require step-by-step analysis. This often exceeds the output context length we set for the model. To address this limitation:\n",
    "\n",
    "1. We first attempt to generate a complete response\n",
    "2. If the response is truncated (indicated by stop_reason = \"length\"), we:\n",
    "   - Concatenate the partial response to the original prompt\n",
    "   - Make another API call with `continuation=True`\n",
    "   - This sets `add_generation_prompt=False` in the tokenizer call\n",
    "3. This process continues until we get a complete response\n",
    "\n",
    "This approach ensures we capture the model's complete reasoning process while maintaining coherence throughout the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfe60b-6fb8-4ade-a40f-52aaf592e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_generate(messages, **kwargs):\n",
    "    \"\"\"\n",
    "    Handle longer responses that exceed token limit\n",
    "    \n",
    "    Parameters:\n",
    "        messages (list): List of message dictionaries\n",
    "        **kwargs: Additional parameters for generate function\n",
    "    \n",
    "    Returns:\n",
    "        dict: Enhanced response including thinking process and final answer\n",
    "    \"\"\"\n",
    "    res = generate(messages, **kwargs)\n",
    "    while res[\"stop_reason\"] == \"length\":\n",
    "        for v in messages:\n",
    "            if v.get(\"role\") == \"user\":\n",
    "               v[\"content\"] += res[\"generation\"]\n",
    "        res = generate(messages, **kwargs, continuation=True)\n",
    "\n",
    "    for v in messages:\n",
    "        if v.get(\"role\") == \"user\":\n",
    "           gen = v[\"content\"] + res[\"generation\"]\n",
    "           answer = gen.split(\"</think>\")[-1]\n",
    "           think = gen.split(\"</think>\")[0].split(\"<think>\")[-1]\n",
    "           res = {**res, \"generation\": gen, \"answer\": answer, \"think\": think}\n",
    "           return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15a690-1d6a-4b31-ac67-1975ede304ff",
   "metadata": {},
   "source": [
    "### Usage Examples\n",
    "#### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874032c-1f2f-483f-81da-62bcdab2adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"Given the following financial data:\n",
    "- Company A's revenue grew from $10M to $15M in 2023\n",
    "- Operating costs increased by 20%\n",
    "- Initial operating costs were $7M\n",
    "\n",
    "Calculate the company's operating margin for 2023. Please reason step by step.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "response = generate(messages)\n",
    "print(\"Model Response:\")\n",
    "print(response[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634701c-1485-4038-bef3-83dbec966fc8",
   "metadata": {},
   "source": [
    "#### Advanced Usage with Complex Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586afd6-a4f0-4ee8-bb2f-82a48fb9bc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"Solve the following optimization problem:\n",
    "\n",
    "A manufacturing company produces two types of products: A and B. \n",
    "They need to determine the optimal production quantities to maximize profit.\n",
    "\n",
    "Given constraints:\n",
    "1. Manufacturing capacity: 60 hours per week\n",
    "2. Product A takes 4 hours to produce\n",
    "3. Product B takes 3 hours to produce\n",
    "4. Storage space can hold maximum 20 units total\n",
    "5. Profit per unit:\n",
    "   - Product A: $200\n",
    "   - Product B: $150\n",
    "6. Minimum required production:\n",
    "   - At least 3 units of Product A\n",
    "   - At least 2 units of Product B\n",
    "\n",
    "Please:\n",
    "1. Set up the linear programming equations\n",
    "2. Solve step by step\n",
    "3. Verify all constraints are met\n",
    "4. Calculate maximum profit\n",
    "5. Analyze sensitivity to changes in constraints\n",
    "6. Recommend optimal production plan\n",
    "\n",
    "Show all your work and reasoning at each step.\"\"\"\n",
    "\n",
    "# System prompt to encourage detailed mathematical reasoning\n",
    "system_prompt = \"\"\"You are a mathematical optimization expert. \n",
    "Please provide detailed step-by-step solutions showing:\n",
    "- All equations and their development\n",
    "- Each calculation step\n",
    "- Verification of constraints\n",
    "- Clear reasoning for each decision\n",
    "- Visual representations where helpful\"\"\"\n",
    "\n",
    "# Run the analysis with auto_generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": complex_prompt}\n",
    "]\n",
    "\n",
    "response = auto_generate(messages, temperature=0.7, max_tokens=4096, top_p=0.9)\n",
    "\n",
    "# Display the response\n",
    "print(\"\\n=== Thinking Process ===\")\n",
    "display(Markdown(response[\"think\"]))\n",
    "print(\"\\n=== Solution ===\")\n",
    "display(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba0f134",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a secure end-to-end process for importing DeepSeek's distilled Llama models to Amazon Bedrock using Custom Model Import (CMI). By implementing Protect AI Guardian throughout our workflow, we've established a \"zero-trust\" security posture for our ML pipeline.\n",
    "\n",
    "We began by using Guardian Gateway to safely download the model from HuggingFace, automatically scanning it for potential vulnerabilities during transfer. This critical first step ensures that third-party models don't introduce security risks to your environment from the outset.\n",
    "\n",
    "As we moved the model through our internal infrastructure and made modifications, we followed zero-trust principles by using Guardian Scanner to continuously validate the model's security posture. This ongoing verification caught potential issues like deserialization vulnerabilities before they could reach production, demonstrating why continuous scanning is essential whenever models are moved or modified.\n",
    "\n",
    "Only after confirming our model passed all security checks did we proceed with creating a CMI job to upload to Amazon Bedrock Custom Model Import. This security-first approach should be considered best practice for any organization working with external AI models or developing models internally.\n",
    "\n",
    "While we've used the DeepSeek-R1-Distill-Llama-8B model in this example, the same secure process applies to other variants including the 70B model. Regardless of the model size or source, always scan external models using Guardian Gateway and continuously validate models with Guardian Scanner throughout your ML lifecycle.\n",
    "\n",
    "For more information about Custom Model Import and its features, refer to the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
